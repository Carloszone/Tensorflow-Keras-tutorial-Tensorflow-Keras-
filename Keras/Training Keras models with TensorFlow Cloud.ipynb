{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Keras models with TensorFlow Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "TensorFlow Cloud is a Python package that provides APIs for a seamless transition from local debugging to distributed training in Google Cloud. It simplifies the process of training TensorFlow models on the cloud into a single, simple function call, requiring minimal setup and no changes to your model. TensorFlow Cloud handles cloud-specific tasks such as creating VM instances and distribution strategies for your models automatically. This guide will demonstrate how to interface with Google Cloud through TensorFlow Cloud, and the wide range of functionality provided within TensorFlow Cloud. We'll start with the simplest use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll get started by installing TensorFlow Cloud, and importing the packages we will need in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_cloud as tfc\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API overview: a first end-to-end example\n",
    "Let's begin with a Keras model training script, such as the following CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 29s 67ms/step - loss: 0.6526 - sparse_categorical_accuracy: 0.8026 - val_loss: 0.0774 - val_sparse_categorical_accuracy: 0.9795\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 22s 51ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.0540 - val_sparse_categorical_accuracy: 0.9845\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 23s 54ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.0500 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0393 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0436 - val_sparse_categorical_accuracy: 0.9865\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0360 - val_sparse_categorical_accuracy: 0.9905\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0329 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 24s 58ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.0349 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0314 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.0340 - val_sparse_categorical_accuracy: 0.9900\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 24s 57ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.0393 - val_sparse_categorical_accuracy: 0.9903\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 24s 57ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9913\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0350 - val_sparse_categorical_accuracy: 0.9912 loss: 0.0104 - sparse_categorical_accuracy:  - ETA: 7s - loss: 0.0104 - sparse_cat - ETA: 6s - loss: 0.0104\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 26s 62ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0441 - val_sparse_categorical_accuracy: 0.9897\n",
      "Epoch 15/20\n",
      "422/422 [==============================] - 24s 58ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.0356 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 16/20\n",
      "422/422 [==============================] - 24s 57ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.0398 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "422/422 [==============================] - 24s 58ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0473 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 18/20\n",
      "422/422 [==============================] - 24s 57ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 19/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 20/20\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0513 - val_sparse_categorical_accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8059c4af0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28)),\n",
    "        # Use a Rescaling layer to make sure input values are in the [0, 1] range.\n",
    "        layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "        # The original images have shape (28, 28), so we reshape them to (28, 28, 1)\n",
    "        layers.Reshape(target_shape=(28, 28, 1)),\n",
    "        # Follow-up with a classic small convnet\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model on Google Cloud we just need to add a call to run() at the beginning of the script, before the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to worry about cloud-specific tasks such as creating VM instances and distribution strategies when using TensorFlow Cloud. The API includes intelligent defaults for all the parameters -- everything is configurable, but many models can rely on these defaults.\n",
    "\n",
    "Upon calling run(), TensorFlow Cloud will:\n",
    "\n",
    "1. Make your Python script or notebook distribution-ready.\n",
    "2. Convert it into a Docker image with required dependencies.\n",
    "3. Run the training job on a GCP GPU-powered VM.\n",
    "4. Stream relevant logs and job information.\n",
    "\n",
    "The default VM configuration is 1 chief and 0 workers with 8 CPU cores and 1 Tesla T4 GPU.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud configuration\n",
    "In order to facilitate the proper pathways for Cloud training, you will need to do some first-time setup. If you're a new Google Cloud user, there are a few preliminary steps you will need to take:\n",
    "\n",
    "1. Create a GCP Project;\n",
    "2. Enable AI Platform Services;\n",
    "3. Create a Service Account;\n",
    "4. Download an authorization key;\n",
    "5. Create a Cloud Storage bucket.\n",
    "\n",
    "Detailed first-time setup instructions can be found in the TensorFlow Cloud README, and an additional setup example is shown on the TensorFlow Blog.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common workflows and Cloud storage\n",
    "In most cases, you'll want to retrieve your model after training on Google Cloud. For this, it's crucial to redirect saving and loading to Cloud Storage while training remotely. We can direct TensorFlow Cloud to our Cloud Storage bucket for a variety of tasks. The storage bucket can be used to save and load large training datasets, store callback logs or model weights, and save trained model files. To begin, let's configure fit() to save the model to a Cloud Storage, and set up TensorBoard monitoring to track training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(28, 28)),\n",
    "            layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "            layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            layers.MaxPooling2D(2),\n",
    "            layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            layers.MaxPooling2D(2),\n",
    "            layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the TensorBoard logs and model checkpoints generated during training in our cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Note: Please change the gcp_bucket to your bucket name.\n",
    "gcp_bucket = \"keras-examples\"\n",
    "\n",
    "checkpoint_path = os.path.join(\"gs://\", gcp_bucket, \"mnist_example\", \"save_at_{epoch}\")\n",
    "\n",
    "tensorboard_path = os.path.join(  # Timestamp included to enable timeseries graphs\n",
    "    \"gs://\", gcp_bucket, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    # TensorBoard will store logs for each epoch and graph performance for us.\n",
    "    keras.callbacks.TensorBoard(log_dir=tensorboard_path, histogram_freq=1),\n",
    "    # ModelCheckpoint will save models after each epoch for retrieval later.\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path),\n",
    "    # EarlyStopping will terminate training when val_loss ceases to improve.\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "]\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will load our data from Keras directly. In general, it's best practice to store your dataset in your Cloud Storage bucket, however TensorFlow Cloud can also accomodate datasets stored locally. That's covered in the Multi-file section of this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow Cloud API provides the remote() function to determine whether code is being executed locally or on the cloud. This allows for the separate designation of fit() parameters for local and remote execution, and provides means for easy debugging without overloading your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 29s 30ms/step - loss: 0.4964 - sparse_categorical_accuracy: 0.8502\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 29s 31ms/step - loss: 0.0616 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 30s 32ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9872\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9899\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 28s 30ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c805040fa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tfc.remote():\n",
    "    epochs = 100\n",
    "    callbacks = callbacks\n",
    "    batch_size = 128\n",
    "else:\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    callbacks = None\n",
    "\n",
    "model.fit(x_train, y_train, epochs=epochs, callbacks=callbacks, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model in GCS after the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"gs://\", gcp_bucket, \"mnist_example\")\n",
    "\n",
    "if tfc.remote():\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this storage bucket for Docker image building, instead of your local Docker instance. For this, just add your bucket to the docker_image_bucket_name parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run(docker_image_bucket_name=gcp_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can load the saved model and view our TensorBoard logs to monitor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "File system scheme 'gs' not implemented (file: 'gs://keras-examples\\mnist_example\\saved_model.pb')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c3d3024b9576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     94\u001b[0m   \u001b[1;31m# Parse the SavedModel protocol buffer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaved_model_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSavedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_pb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m       \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_pb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mfile_exists\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPropagates\u001b[0m \u001b[0many\u001b[0m \u001b[0merrors\u001b[0m \u001b[0mreported\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFileSystem\u001b[0m \u001b[0mAPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfile_exists_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mfile_exists_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m   \"\"\"\n\u001b[0;32m    267\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0m_pywrap_file_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: File system scheme 'gs' not implemented (file: 'gs://keras-examples\\mnist_example\\saved_model.pb')"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir \"gs://keras-examples-jonah/logs/fit\" --name \"Guide MNIST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large-scale projects\n",
    "In many cases, your project containing a Keras model may encompass more than one Python script, or may involve external data or specific dependencies. TensorFlow Cloud is entirely flexible for large-scale deployment, and provides a number of intelligent functionalities to aid your projects.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry points: support for Python scripts and Jupyter notebooks\n",
    "Your call to the run() API won't always be contained inside the same Python script as your model training code. For this purpose, we provide an entry_point parameter. The entry_point parameter can be used to specify the Python script or notebook in which your model training code lives. When calling run() from the same script as your model, use the entry_point default of None.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pip dependencies\n",
    "If your project calls on additional pip dependencies, it's possible to specify the additional required libraries by including a requirements.txt file. In this file, simply put a list of all the required dependencies and TensorFlow Cloud will handle integrating these into your cloud build.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python notebooks\n",
    "TensorFlow Cloud is also runnable from Python notebooks. Additionally, your specified entry_point can be a notebook if needed. There are two key differences to keep in mind between TensorFlow Cloud on notebooks compared to scripts:\n",
    "\n",
    "1. When calling run() from within a notebook, a Cloud Storage bucket must be specified for building and storing your Docker image.\n",
    "2. GCloud authentication happens entirely through your authentication key, without project specification. An example workflow using TensorFlow Cloud from a notebook is provided in the \"Putting it all together\" section of this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-file projects\n",
    "If your model depends on additional files, you only need to ensure that these files live in the same directory (or subdirectory) of the specified entry point. Every file that is stored in the same directory as the specified entry_point will be included in the Docker image, as well as any files stored in subdirectories adjacent to the entry_point. This is also true for dependencies you may need which can't be acquired through pip\n",
    "\n",
    "For an example of a custom entry-point and multi-file project with additional pip dependencies, take a look at this multi-file example on the TensorFlow Cloud Repository. For brevity, we'll just include the example's run() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    entry_point=\"train_model.py\",\n",
    "    requirements=\"requirements.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine configuration and distributed training\n",
    "\n",
    "Model training may require a wide range of different resources, depending on the size of the model or the dataset. When accounting for configurations with multiple GPUs, it becomes critical to choose a fitting distribution strategy. Here, we outline a few possible configurations:\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-worker distribution\n",
    "Here, we can use COMMON_MACHINE_CONFIGS to designate 1 chief CPU and 4 worker GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    chief_config=tfc.COMMON_MACHINE_CONFIGS['CPU'],\n",
    "    worker_count=2,\n",
    "    worker_config=tfc.COMMON_MACHINE_CONFIGS['T4_4X']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, TensorFlow Cloud chooses the best distribution strategy for your machine configuration with a simple formula using the chief_config, worker_config and worker_count parameters provided.\n",
    "\n",
    "1. If the number of GPUs specified is greater than zero, tf.distribute.MirroredStrategy will be chosen.\n",
    "2. If the number of workers is greater than zero, tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.experimental.TPUStrategy will be chosen based on the accelerator type.\n",
    "3. Otherwise, tf.distribute.OneDeviceStrategy will be chosen.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU distribution\n",
    "Let's train the same model on TPU, as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    chief_config=tfc.COMMON_MACHINE_CONFIGS[\"CPU\"],\n",
    "    worker_count=1,\n",
    "    worker_config=tfc.COMMON_MACHINE_CONFIGS[\"TPU\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom distribution strategy\n",
    "To specify a custom distribution strategy, format your code normally as you would according to the distributed training guide and set distribution_strategy to None. Below, we'll specify our own distribution strategy for the same MNIST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "  model = create_model()\n",
    "\n",
    "if tfc.remote():\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "else:\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    callbacks = None\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=epochs, callbacks=callbacks, batch_size=batch_size\n",
    ")\n",
    "\n",
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    chief_config=tfc.COMMON_MACHINE_CONFIGS['CPU'],\n",
    "    worker_count=2,\n",
    "    worker_config=tfc.COMMON_MACHINE_CONFIGS['T4_4X'],\n",
    "    distribution_strategy=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Docker images\n",
    "By default, TensorFlow Cloud uses a Docker base image supplied by Google and corresponding to your current TensorFlow version. However, you can also specify a custom Docker image to fit your build requirements, if necessary. For this example, we will specify the Docker image from an older version of TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    base_docker_image=\"tensorflow/tensorflow:2.1.0-gpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional metrics\n",
    "You may find it useful to tag your Cloud jobs with specific labels, or to stream your model's logs during Cloud training. It's good practice to maintain proper labeling on all Cloud jobs, for record-keeping. For this purpose, run() accepts a dictionary of labels up to 64 key-value pairs, which are visible from the Cloud build logs. Logs such as epoch performance and model saving internals can be accessed using the link provided by executing tfc.run or printed to your local terminal using the stream_logs flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_labels = {\"job\": \"mnist-example\", \"team\": \"keras-io\", \"user\": \"jonah\"}\n",
    "\n",
    "tfc.run(\n",
    "    docker_image_bucket_name=gcp_bucket,\n",
    "    job_labels=job_labels,\n",
    "    stream_logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "For an in-depth Colab which uses many of the features described in this guide, follow along this example to train a state-of-the-art model to recognize dog breeds from photos using feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
