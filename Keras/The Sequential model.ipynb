{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequential model\n",
    "# 顺序模型/序贯模型（以下统称顺序模型）\n",
    "\n",
    "TensorFlow guide link: [here](https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "TensorFlow [教程链接](https://www.tensorflow.org/guide/keras/sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "# 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load module we need\n",
    "# 加载模组/包\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use a Sequential model\n",
    "# 何时应该使用顺序模型？\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "顺序模型非常适合需要多个层堆叠，且每个层只有一个输入张量和输出张量的场景\n",
    "\n",
    "A Sequential model is not appropriate when:\n",
    "1. Your model has multiple inputs or multiple outputs\n",
    "2. Any of your layers has multiple inputs or multiple outputs\n",
    "3. You need to do layer sharing\n",
    "4. You want non-linear topology (e.g. a residual connection, a multi-branch model)\n",
    "\n",
    "顺序模型不适合于以下情况：\n",
    "1. 你的模型有多个输入和输出（张量）\n",
    "2. 模型的任意层存在多个输入和输出（张量）\n",
    "3. 你需要对层权值进行共享（如卷积网络）\n",
    "4. 你的模型存在非线性拓扑结构（如残差连接，多分支模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways to build sequential model\n",
    "# 两种建立顺序模型的方法\n",
    "\n",
    "# 1\n",
    "# Define Sequential model with 3 layers\n",
    "# 定义一个3层的顺序模型\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Call model on a test input\n",
    "# 通过测试用输入调用模型\n",
    "x = tf.ones((3,3))\n",
    "y = model(x)\n",
    "\n",
    "#2\n",
    "# Create 3 layers model\n",
    "# 定义模型的三个层级\n",
    "layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
    "layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
    "layer3 = layers.Dense(4, name=\"layer3\")\n",
    "\n",
    "# Call layers on a test input\n",
    "# 通过测试用输入调用模型\n",
    "x = tf.ones((3, 3))\n",
    "y = layer3(layer2(layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 学习笔记\n",
    "\n",
    "### the first way\n",
    "The first way follows the classical neural network [image](https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg) order: input - layer1 - layer2 - layer 3 - output \n",
    "\n",
    "### 第一种方法\n",
    "第一种方法遵循了[经典的神经网络结构](https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg)，按照“输入层，隐藏层1，隐藏层2，影藏层3，输出层”的顺序构建模型\n",
    "\n",
    "### the second way\n",
    "The second way follows the mathetical theory: the output previous layer is the input of the following layer. So, it is layer3(layer2(layer1(x)))\n",
    "\n",
    "### 第二种方法\n",
    "第二种方法体现了数学上的函数嵌套思想，前一层的输出是下一层的输入，所以表达成layer3(layer2(layer1(x)))\n",
    "\n",
    "### Fuction\n",
    "1.**keras.Sequential()** Like basic machine learning functions(e.g. LogisticRegression()), it is used to implement a certain model. More detail see [here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "2.**layers.dense()** This function is used to create the layer. In the previous cases, the first parameter determine the node number; the second parameter, \"activation\", means the activation function; and the final parameter is the name of the layer. More detail see [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n",
    "3.**tf.ones()**  This function will creates a tensor with all elements set to 1. tf.noes() is to tensor what np.zeros() is to array. see [here](https://www.tensorflow.org/api_docs/python/tf/ones).\n",
    "\n",
    "### 涉及的函数\n",
    "1.**keras.Sequential()** 类似基础的机器学习函数（如LogisticRegression()），这个函数用于声明/定义一个特定的模型。更多信息见[此]()\n",
    "\n",
    "2.**layers.dense()** 这个函数用于创建模型的层。在之前的示例中，第一个参数设定了该层的节点数；第二个参数定义了该层的激活函数；第三个参数用于标注该层的名字。更多信息见[此]()\n",
    "\n",
    "3.**tf.ones()**  这个函数用于生成一个所有元素都是1的张量。tf.noes()之于张量，正如np.zeros()之于数组\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Sequential model\n",
    "# 创建一个顺序模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x15e67039670>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x15e66ff3400>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x15e66ff37c0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Sequential model by passing a list of layers to the Sequential constructor\n",
    "# 通过传递层的列表来创建顺序模型\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(4),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# check layer information via the layers attribute\n",
    "# 通过layers属性来查看层信息\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Sequential model by add() method\n",
    "# 通过add()方法来创建顺序模型\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(4))\n",
    "\n",
    "# remove layers by pop() method\n",
    "# pop()方法来创建顺序模型\n",
    "model.pop()\n",
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name model or layer by name argument\n",
    "# 通过name参数对模型或层进行命名\n",
    "model = keras.Sequential(name=\"my_sequential\")\n",
    "model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\"))\n",
    "model.add(layers.Dense(4, name=\"layer3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 学习笔记\n",
    "this part shows some basic operation of Sequential model such as add/remove layer, check model layer information, and annotate name/label via name argument\n",
    "\n",
    "这一部分展示了一些创建顺序模型的基本操作，包括添加/删除层， 查看模型的层信息，以及通过参数对模型和层进行命名与注释\n",
    "\n",
    "### 1. two ways to add layer\n",
    "1. passing a list of layers to model\n",
    "2. using add() method to add layers one by one\n",
    "\n",
    "### 2. check layer information via layers attribute\n",
    "\n",
    "### 3. remove layer via pop() method.\n",
    "there is a differece between model.pop() and list.pop(). In list, pop() will remove the last value and return it, the model.pop() just remove the last layer. It doesn't return the remove layer\n",
    "\n",
    "### 4. annotate model or layer\n",
    "It is very easy to add annotation by name argument. I recommend to annotate them with the goal or order for each model/layer because when I review my code I wrote several weeks ago, sometimes it is hard for me to understand what I want to do at that timing point without annotation.\n",
    "\n",
    "### 1. 两种添加层的方式\n",
    "1. 通过层列表加入模型\n",
    "2. 使用add()方法以此加入层\n",
    "\n",
    "### 2 通过layers属性查看层信息\n",
    "\n",
    "### 3. 通过pop()方法移除层\n",
    "列表对象也有pop()方法。两者的不同之处在于，对列表对象而言，pop()不但会移除最后的值，还会输出改值；而对于tensor model, pop()只会移除最后的层，不会输出这个层。\n",
    "\n",
    "### 4. 对模型和层进行注释/命名\n",
    "通过name参数很容易对model和层进行注释或命名，我建议用目的或者次序对模型和层进行注释， 因为我时常会回顾我之前写的代码，我发现在没有注释的情况下，理解自己当时的想法是一件非常困难的事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the input shape in advance\n",
    "# 预先指定输入的维度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the layer weights are: []\n",
      "after call layer, the layer.weights are: [<tf.Variable 'dense_21/kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.15977407,  0.5230683 ,  0.07423282],\n",
      "       [-0.00153917,  0.66534936,  0.74067616],\n",
      "       [-0.36553162, -0.24939686, -0.68069667],\n",
      "       [-0.9184909 ,  0.625263  , -0.13357067]], dtype=float32)>, <tf.Variable 'dense_21/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# check and compare the layer weights\n",
    "# 检查并比较层级的权重\n",
    "# 直接查看\n",
    "layer = layers.Dense(3)\n",
    "print('the layer weights are:', layer.weights)\n",
    "\n",
    "# Call layer on a test input\n",
    "#调用函数后查看\n",
    "x = tf.ones((1, 4))\n",
    "y = layer(x)\n",
    "print('after call layer, the layer.weights are:', layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights after calling the model: 6\n"
     ]
    }
   ],
   "source": [
    "# check and compare the model weights\n",
    "# 检查并比较层级的权重\n",
    "\n",
    "# 只定义函数\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(4),\n",
    "    ]\n",
    ")  # No weights at this stage!\n",
    "\n",
    "# you will get an error when you run the following two lines code\n",
    "# 此时如果调用weight属性或者summary()方法，会报错。 因为此时层的权重尚未确定\n",
    "# model.weights\n",
    "# model.summary()\n",
    "\n",
    "# Call the model on a test input\n",
    "# 调用模型后再次检查层级的权重信息\n",
    "x = tf.ones((1, 4))\n",
    "y = model(x)\n",
    "print(\"Number of weights after calling the model:\", len(model.weights))  # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (1, 2)                    10        \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (1, 3)                    9         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (1, 4)                    16        \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now (after build the model), the summary() works\n",
    "# 在建立模型后，summary()方法可以正常工作\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an Input object to your model, so that it knows its input shape from the start:\n",
    "\n",
    "需要指出的是，如果可以在渐进式的构建模型的同时，查看模型的概览信息（包括层信息），将非常有利于我们的建模工作。在接下来的示例中， 你将创建一个模型并且同时将输入对象传递给你的模型，这样你的模型就能知道输入对象的维度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(4,))) # passing input object when add layer 当你添加层时，同时传递输入对象信息\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 学习笔记\n",
    "Explanation of previous result\n",
    "\n",
    "对上述输出表格的解释说明\n",
    "\n",
    "**Layer(type)** shows the layer name and type\n",
    "\n",
    "**Output Shape** shows the output information. In the previous example, the value is (None, 2). None is output type, and 2 is the number of output: the layer has two nodes, so there are 2 outputs\n",
    "\n",
    "\n",
    "**Param** the number of param is based on the input. in the previous case, the formula is param = (input_number + 1) * node_number. 1 is the bias.\n",
    "\n",
    "**Layer(type)** 这个字段显示了模型中层的名字和类型\n",
    "\n",
    "**Output Shape** 这一字段显示了输出的维度。在上述例子中，值为（None,2）. None是输出的类型（并没有具体的值输入，所以输出为None）， 2是指输出的数量。 这个层含有两个节点，所以有两个输出\n",
    "\n",
    "**Param** 这个字段显示了层的权重数量。权重数量与输入的维度有关。在上述例子中，权重数量的公式为 权重数量 = （输入维度+1）*层的节点数量。 其中的1是指偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x15e6758e790>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check layer information\n",
    "# 再次检查层信息\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A simple alternative is to just pass an input_shape argument to your first layer\n",
    "# 一个简单的，传递输入信息的替代方案\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\", input_shape=(4,)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n",
    "\n",
    "#### In general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n",
    "\n",
    "### 如之前的案例所示，只有预先定义了输入维度的模型才有权重信息和输出的维度信息\n",
    "\n",
    "### 一般情况下，如果知道输入的维度信息，应当在建模时预先设定输入的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 学习笔记\n",
    "\n",
    "1. only when a model is \"built\" , it will get the weights. specifing the input shape of model in advance can let us check the model information, such as the layer number, parameter number and output shape, before calling or training model \n",
    "\n",
    "2. usually, we call input as \"input layer\", but remember, in TensorFlow/Keras, the input object isn't a layer\n",
    "\n",
    "1. 只有当一个模型“建立”完毕后（即获得输入信息后），它才有权重信息。预先设定好输入的维度信息可以让我们在调研或者训练模型之前就查看模型信息，比如模型的层数量，参数数量以及输出的维度。\n",
    "\n",
    "2. 一般来说，我们会称呼输入为“输入层”，但是请记住在TensorFlow/Keras中，输入对象并不是一个层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A common debugging workflow: add() + summary()\n",
    "# 一个常规的调试流程： add() + summary()\n",
    "\n",
    "When building a new Sequential architecture, it's useful to incrementally stack layers with add() and frequently print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n",
    "\n",
    "当构建一个新的顺序结构后，通过add()方法逐步加入层并在这一过程中频繁地通过summary()方法查看模型信息非常有用。 例如，你可以观察到Conv2D和MaxPooling2D层时如何对图像的特征图进行降采样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 123, 123, 32)      2432      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 121, 121, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 40, 40, 32)        0         \n",
      "=================================================================\n",
      "Total params: 11,680\n",
      "Trainable params: 11,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(250, 250, 3)))  # 250x250 RGB images 这是一个规格为250x250的RGB图像\n",
    "model.add(layers.Conv2D(32, 5, strides=2, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "# Can you guess what the current output shape is at this point? Probably not.\n",
    "# 你能猜到目前的输出的维度吗？ 这不太容易\n",
    "\n",
    "# Let's just print it:\n",
    "# 我们直接输出这些信息\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 123, 123, 32)      2432      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 121, 121, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 38, 38, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 36, 36, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "=================================================================\n",
      "Total params: 48,672\n",
      "Trainable params: 48,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The answer was: (40, 40, 32), so we can keep downsampling...\n",
    "# 结果显示目前的输入维度为(40, 40, 32)，因此我们继续重复降采样步骤\n",
    "\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "# And now?\n",
    "# 检查目前的模型信息\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have 4x4 feature maps, time to apply global max pooling.\n",
    "# 现在我们获得了一个4x4的特征图，是时候应用全局最大化池化了\n",
    "model.add(layers.GlobalMaxPooling2D())\n",
    "\n",
    "# Finally, we add a classification layer.\n",
    "# 最后，我们加入一个分类层\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 学习笔记\n",
    "1. uesing add() method to add layers incrementally\n",
    "\n",
    "2. when add a set of layers, we can use the summary() method to check the model\n",
    "\n",
    "3. following this debugging workflow, we can easily locate the layer set contained bugs.\n",
    "\n",
    "\n",
    "1. 利用add()方法渐进式地往模型中加入层\n",
    "\n",
    "2. 当 加入一系列新层后，我们可以通过summary（）方法来检查模型\n",
    "\n",
    "3. 遵循这一调试流程，我们也可以很轻松地定位到那些存在bug的层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do once you have a model\n",
    "# 当我有了一个模型，那么下一步是？\n",
    "\n",
    "Once your model architecture is ready, you will want to:\n",
    "\n",
    "1. Train your model, evaluate it, and run inference. See our [guide to training & evaluation with the built-in loops](https://www.tensorflow.org/guide/keras/train_and_evaluate/)\n",
    "2. Save your model to disk and restore it. See our [guide to serialization & saving.](https://www.tensorflow.org/guide/keras/save_and_serialize/)\n",
    "3. Speed up model training by leveraging multiple GPUs. See our [guide to multi-GPU and distributed training.](https://keras.io/guides/distributed_training/)\n",
    "\n",
    "\n",
    "一旦你的模型架构准备完毕，你也许想要做：\n",
    "1. 训练、评估模型，并进行预测。 请参见这篇[指南](https://www.tensorflow.org/guide/keras/train_and_evaluate/)\n",
    "\n",
    "2. 讲模型保存到本地。请参见这篇[指南](https://www.tensorflow.org/guide/keras/save_and_serialize/)\n",
    "\n",
    "3. 利用GPU加速节约训练时间。请参见这篇[指南](https://keras.io/guides/distributed_training/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction with a Sequential model\n",
    "# 顺序模型的特征提取\n",
    "\n",
    "Once a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n",
    "\n",
    "一旦顺序模型被建立，它会像一个函数式API一般运作。 这意味着每一层都会有输入和输出属性。这些属性可以用来最一些有趣的事情，比如构建一个新模型用于提取这个顺序模型的所有中间层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from whole model layers\n",
    "# 从模型的所有层中提取特征\n",
    "\n",
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=initial_model.inputs,\n",
    "    outputs=[layer.output for layer in initial_model.layers],\n",
    ")\n",
    "\n",
    "# Call feature extractor on test input.\n",
    "# 通过测试用输入来调用模型“feature extractor”\n",
    "x = tf.ones((1, 250, 250, 3))\n",
    "features = feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from one layer\n",
    "# 从特定层中提取特征\n",
    "\n",
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\", name=\"my_intermediate_layer\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=initial_model.inputs,\n",
    "    outputs=initial_model.get_layer(name=\"my_intermediate_layer\").output,\n",
    ")\n",
    "# Call feature extractor on test input.\n",
    "# 通过测试用输入来调用模型“feature extractor”\n",
    "x = tf.ones((1, 250, 250, 3))\n",
    "features = feature_extractor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note 学习笔记\n",
    "**the difference between two examples is the feature_extractor()**\n",
    "\n",
    "In the first example, it use a for loop to visit all layers; \n",
    "\n",
    "In the second example, it use get_layer() to visit the target layer\n",
    "\n",
    "**两个示例的不同之处在于feature_extractor模型**\n",
    "\n",
    "在第一个示例中，新的模型采用了for循环来遍历所有层并提取特征\n",
    "\n",
    "在第二个示例中，新的模型利用get_layer()方法来制定需要提取特征的层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with a Sequential model\n",
    "# 顺序模型的迁移学习\n",
    "\n",
    "Transfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our [guide to transfer learning.](https://www.tensorflow.org/guide/keras/transfer_learning/)\n",
    "\n",
    "Here are two common transfer learning blueprint involving Sequential models.\n",
    "\n",
    "First, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model.layers and set layer.trainable = False on each layer, except the last one. Like this:\n",
    "\n",
    "迁移学习包括冻结模型底层的层，只训练模型顶层的层。 如果你对迁移学习的概念并不熟悉，请阅读这篇[指南](https://www.tensorflow.org/guide/keras/transfer_learning/)\n",
    "\n",
    "对于顺序模型而言，有两种常见的迁移学习实现方法\n",
    "\n",
    "首先，我们假设你有一个顺序模型，并且你希望冻结除最后一个层以外的所有层。在这个案例中，你所需要做的是简单地对模型中的层进行循环迭代，将除了最后一层以外的所有层设置为“layer.trainable = False”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Squential moddel\n",
    "# 创建一个顺序模型\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(784)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "# Presumably you would want to first load pre-trained weights.\n",
    "# 如果你需要预先载入权重，可以使用下面的代码\n",
    "#model.load_weights(...)\n",
    "\n",
    "# Freeze all layers except the last one.\n",
    "# 冻结除最后一层以外的所有层\n",
    "for layer in model.layers[:-1]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Recompile and train (this will only update the weights of the last layer).\n",
    "# 汇编并训练模型（只有最后一层的权重会改变，其他层的权重保持不变）\n",
    "#model.compile(...)\n",
    "#model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 40s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Another common blueprint\n",
    "# 另一个实现方法\n",
    "\n",
    "# Load a convolutional base with pre-trained weights\n",
    "# 直接载入一个卷积模型和模型权重\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg')\n",
    "\n",
    "# Freeze the base model\n",
    "# 直接冻结这个基础模型\n",
    "base_model.trainable = False\n",
    "\n",
    "# Use a Sequential model to add a trainable classifier on top\n",
    "# 利用顺序模型将前面的卷积模型和一个可训练的分类器层组合起来\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Dense(1000),\n",
    "])\n",
    "\n",
    "# Compile & train\n",
    "# 汇编并训练新模型\n",
    "#model.compile(...)\n",
    "#model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note 学习笔记\n",
    "**The difference between two blueprint**\n",
    "\n",
    "1. The first blueprint is layer level operation. it seem like \n",
    "\n",
    "Model = (layer1(don't run) , layer2(don't run), layer3(don't run), ... layerN(run!))\n",
    "\n",
    "2. The second blueprint is model level operation. it wraps part of layers as model, then freeze the model. Or it just freeze the model directly. Next, add all model to the Sequential model. \n",
    "\n",
    "\n",
    "3. the Sequential model looks like a pipeline. It just runs its content based on the setting orderly\n",
    "\n",
    "**两种实现方式的异同**\n",
    "1. 第一种方式是对层进行操作。其过程类似与 Model = (层1（不运行），层2（不运行），...，层N（运行！）)\n",
    "\n",
    "2. 第二种方式是对模型进行操作。先把不运行或运行的层封装成一个新模型，然后冻结这个模型，或者直接冻结不需要进行训练的模型部分。然后，将模型加入到顺序模型之中，这样就可以观察部分模型的运作情况\n",
    "\n",
    "3. 顺序模型在这一过程中的角色类似与管道。 它只是根据设定按次序执行/不执行内部的模型或层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
