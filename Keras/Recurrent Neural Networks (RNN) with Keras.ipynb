{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b518b04cbfe0"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ca65cda94c8"
   },
   "source": [
    "# Recurrent Neural Networks (RNN) with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6873211b02d4"
   },
   "source": [
    "## Introduction\n",
    "## 简介\n",
    "\n",
    "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
    "modeling sequence data such as time series or natural language.\n",
    "\n",
    "循环神经网络（RNN）是一种非常适合序列数据（如时间序列或自然语言）建模的神经网络\n",
    "\n",
    "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
    "sequence, while maintaining an internal state that encodes information about the\n",
    "timesteps it has seen so far.\n",
    "\n",
    "原理上，RNN层通过for循环迭代序列数据上的时间段，同时保持一个内部状态来编码它所看到的时间段内的信息\n",
    "\n",
    "The Keras RNN API is designed with a focus on:\n",
    "\n",
    "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
    "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
    "having to make difficult configuration choices.\n",
    "\n",
    "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
    "part of the `for` loop) with custom behavior, and use it with the generic\n",
    "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
    "prototype different research ideas in a flexible way with minimal code.\n",
    "\n",
    "Keras RNN API聚焦于：\n",
    "\n",
    "- **易用**: 内置的 `keras.layers.RNN`, `keras.layers.LSTM`,\n",
    "`keras.layers.GRU` 层可以让你快速地建立循环模型而不用做艰难的设置选择.\n",
    "\n",
    "- **高度自定义**: 你可以自定义你专属的RNN cell层 (内部`for` 循环的一部分), 并与\n",
    "`keras.layers.RNN` 层(`for`循环本身)一同使用. 这允许你只使用少量代码就以一种灵活的方式将不同的研究思路原型化\n",
    "\n",
    "**NOTE** cell层其实就是RNN网络本身，或者说是某一时间段内的RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3600ee25c8e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:07.626882Z",
     "iopub.status.busy": "2021-03-05T18:08:07.626262Z",
     "iopub.status.idle": "2021-03-05T18:08:13.814281Z",
     "shell.execute_reply": "2021-03-05T18:08:13.813686Z"
    },
    "id": "71c626bbac35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4041a2e9b310"
   },
   "source": [
    "## Built-in RNN layers: a simple example\n",
    "## 内置RNN层：一个简单的示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98e0c38cf95d"
   },
   "source": [
    "There are three built-in RNN layers in Keras:\n",
    "\n",
    "1. `keras.layers.SimpleRNN`, a fully-connected RNN where the output from previous\n",
    "timestep is to be fed to next timestep.\n",
    "\n",
    "2. `keras.layers.GRU`, first proposed in\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
    "\n",
    "3. `keras.layers.LSTM`, first proposed in\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "In early 2015, Keras had the first reusable open-source Python implementations of LSTM\n",
    "and GRU.\n",
    "\n",
    "Here is a simple example of a `Sequential` model that processes sequences of integers,\n",
    "embeds each integer into a 64-dimensional vector, then processes the sequence of\n",
    "vectors using a `LSTM` layer.\n",
    "\n",
    "Keras提供了三个内置的RNN层\n",
    "\n",
    "1. `keras.layers.SimpleRNN`, 一个全连接RNN，来自前一个时间段的输出会作为下一个时间段的输入.\n",
    "\n",
    "2. `keras.layers.GRU`, 首次提出是在\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
    "\n",
    "3. `keras.layers.LSTM`, 首次提出是在\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "在2015年初，Keras有了第一个可以复用的开源LSTM和GRU Python实例\n",
    "\n",
    "这里展示一个简单的顺序模型示例：处理一连串整数，将每一个整数嵌入到一个64维向量之中，再用`LSTM`层来处理一连串的向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:13.823039Z",
     "iopub.status.busy": "2021-03-05T18:08:13.818885Z",
     "iopub.status.idle": "2021-03-05T18:08:22.834887Z",
     "shell.execute_reply": "2021-03-05T18:08:22.835389Z"
    },
    "id": "a5617759e54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "# 现在模型添加一个嵌入层，预期输入长度为1000，输出的嵌入向量维度为64维\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "# 添加一个LSTM层，内部有128个节点\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "# 添加一个有10个节点的稠密层\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb8ef33660a0"
   },
   "source": [
    "Built-in RNNs support a number of useful features:\n",
    "\n",
    "- Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n",
    "- Ability to process an input sequence in reverse, via the `go_backwards` argument\n",
    "- Loop unrolling (which can lead to a large speedup when processing short sequences on\n",
    "CPU), via the `unroll` argument\n",
    "- ...and more.\n",
    "\n",
    "For more information, see the\n",
    "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n",
    "\n",
    "内置的RNN层有很多有用的功能：\n",
    "- 循环舍弃, 通过`dropout`和`recurrent_dropout`参数实现\n",
    "- 反向处理输入序列, 通过`go_backwards` 参数实现\n",
    "- 循环展开 (这可以使CPU对短序列输入的处理速度得到明显提升), 通过`unroll`参数实现\n",
    "- ...其他.\n",
    "\n",
    "想要了解更多信息，请参见[RNN API 官方文档](https://keras.io/api/layers/recurrent_layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43aa4e4f344d"
   },
   "source": [
    "## Outputs and states\n",
    "\n",
    "By default, the output of a RNN layer contains a single vector per sample. This vector\n",
    "is the RNN cell output corresponding to the last timestep, containing information\n",
    "about the entire input sequence. The shape of this output is `(batch_size, units)`\n",
    "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
    "\n",
    "A RNN layer can also return the entire sequence of outputs for each sample (one vector\n",
    "per timestep per sample), if you set `return_sequences=True`. The shape of this output\n",
    "is `(batch_size, timesteps, units)`.\n",
    "\n",
    "## 输出与状态\n",
    "\n",
    "默认情况下，RNN层的输出是每一个样本输出一个向量。这个向量是RNN是对应样本的最后时间段的RNN网络的输出，且含有整个输入序列的信息。输出的形式是（批量大小，节点数），此处的节点数是指层构建时所传递的“units”参数\n",
    "\n",
    "RNN层同样可以返回每一个样本的输出序列（即每一个样本在各个时间段的输出）。如果你设置`return_sequences=True`，那么输出的形式是（批量大小，时间段数，节点数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:22.842884Z",
     "iopub.status.busy": "2021-03-05T18:08:22.842158Z",
     "iopub.status.idle": "2021-03-05T18:08:23.105050Z",
     "shell.execute_reply": "2021-03-05T18:08:23.105488Z"
    },
    "id": "c3294dec91e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 256)         247296    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 361,866\n",
      "Trainable params: 361,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "# GRU的输出是一个3D张量，其形式为（批量大小，时间段数，节点数（256））\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "# 简单RNN的输出是一个2D张量，形式是（批量大小，节点数（128））\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "266812a04bb2"
   },
   "source": [
    "In addition, a RNN layer can return its final internal state(s). The returned states\n",
    "can be used to resume the RNN execution later, or\n",
    "[to initialize another RNN](https://arxiv.org/abs/1409.3215).\n",
    "This setting is commonly used in the\n",
    "encoder-decoder sequence-to-sequence model, where the encoder final state is used as\n",
    "the initial state of the decoder.\n",
    "\n",
    "To configure a RNN layer to return its internal state, set the `return_state` parameter\n",
    "to `True` when creating the layer. Note that `LSTM` has 2 state  tensors, but `GRU`\n",
    "only has one.\n",
    "\n",
    "To configure the initial state of the layer, just call the layer with additional\n",
    "keyword argument `initial_state`.\n",
    "Note that the shape of the state needs to match the unit size of the layer, like in the\n",
    "example below.\n",
    "\n",
    "此外，RNN层还可以返回它的最终内部状态，这个状态可以被用于稍后恢复RNN的执行或者[初始化其他的RNN网络](https://arxiv.org/abs/1409.3215).这是做法常见于编码-解码或者序列到序列模型，这里编码的最终状态将作为解码器的最初状态\n",
    "\n",
    "要设置RNN层返回它的最终状态，需要在创建模型时将`return_state` 参数为 `True`。请注意，`LSTM`有两个状态张量，而`GRU`只有一个\n",
    "\n",
    "要设置层的初始化状态，只需要在调用层时添加关键词参数`initial_state`。注意，状态的形式需要和层的节点大小相匹配，就像上面的示例一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:23.113729Z",
     "iopub.status.busy": "2021-03-05T18:08:23.108974Z",
     "iopub.status.idle": "2021-03-05T18:08:23.492078Z",
     "shell.execute_reply": "2021-03-05T18:08:23.491539Z"
    },
    "id": "ece412e6afbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 64)     64000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 64)     128000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder (LSTM)                  [(None, 64), (None,  33024       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  (None, 64)           33024       embedding_2[0][0]                \n",
      "                                                                 encoder[0][1]                    \n",
      "                                                                 encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 258,698\n",
      "Trainable params: 258,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_vocab = 1000\n",
    "decoder_vocab = 2000\n",
    "\n",
    "encoder_input = layers.Input(shape=(None,))\n",
    "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
    "    encoder_input\n",
    ")\n",
    "\n",
    "# Return states in addition to output\n",
    "# 返回输出和状态\n",
    "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
    "    encoder_embedded\n",
    ") # 通过参数‘return_state=True’来返回状态，层的类型是LSTM，所以有两个状态\n",
    "\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_input = layers.Input(shape=(None,))\n",
    "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
    "    decoder_input\n",
    ")\n",
    "\n",
    "# Pass the 2 states to a new LSTM layer, as initial state\n",
    "# 将两个状态作为初始状态传递给一个新的LSTM层\n",
    "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
    "    decoder_embedded, initial_state=encoder_state\n",
    ")\n",
    "output = layers.Dense(10)(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note 学习笔记\n",
    "\n",
    "Compare with the previous model, the summary of LSTM/RNN model has a new column called 'connected to'\n",
    "\n",
    "Basically, there is no many difference between model RNN model or other models in Keras. In the condition of using built-in layers, the building process of model is similar. So, I think how to design a model structure may be difficult than how to build a model.\n",
    "\n",
    "和之前的模型相比，LSTM/RNN模型的概览部分多了一个叫做'connected to'的列\n",
    "\n",
    "基本上来说，在Keras，RNN模型和其他类型的模型没有多少区别。在使用内置层的情况下，构建模型的过程是相似的。所以我认为设计模型结构也许比构建模型要困难"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e97a845a372a"
   },
   "source": [
    "## RNN layers and RNN cells\n",
    "## RNN层和RNN cells\n",
    "\n",
    "In addition to the built-in RNN layers, the RNN API also provides cell-level APIs.\n",
    "Unlike RNN layers, which processes whole batches of input sequences, the RNN cell only\n",
    "processes a single timestep.\n",
    "\n",
    "除了内置RNN层之外，RNN API还提供了cell级别的API。 不同于RNN层（需要处理全部批量的序列输入），RNN cell值需要处理单个时间段内的序列输入\n",
    "\n",
    "\n",
    "The cell is the inside of the `for` loop of a RNN layer. Wrapping a cell inside a\n",
    "`keras.layers.RNN` layer gives you a layer capable of processing batches of\n",
    "sequences, e.g. `RNN(LSTMCell(10))`.\n",
    "\n",
    "cell存在与RNN层中'for'循环的内部。将cell包裹在`keras.layers.RNN`层内部使这个层可以处理批量的序列。例如`RNN(LSTMCell(10))`。\n",
    "\n",
    "Mathematically, `RNN(LSTMCell(10))` produces the same result as `LSTM(10)`. In fact,\n",
    "the implementation of this layer in TF v1.x was just creating the corresponding RNN\n",
    "cell and wrapping it in a RNN layer.  However using the built-in `GRU` and `LSTM`\n",
    "layers enable the use of CuDNN and you may see better performance.\n",
    "\n",
    "从数学角度来看，`RNN(LSTMCell(10))`产生了和`LSTM(10)`相同的结果。实际上，这是实例只是创建了对应的RNN cell并把这个cell包裹在一个RNN层中。 然而，使用内置`GRU`和`LSTM`层可以使用CuDNN，然后你可能得到更好的结果\n",
    "\n",
    "There are three built-in RNN cells, each of them corresponding to the matching RNN\n",
    "layer.\n",
    "\n",
    "- `keras.layers.SimpleRNNCell` corresponds to the `SimpleRNN` layer.\n",
    "\n",
    "- `keras.layers.GRUCell` corresponds to the `GRU` layer.\n",
    "\n",
    "- `keras.layers.LSTMCell` corresponds to the `LSTM` layer.\n",
    "\n",
    "Keras有三个内置的RNN cells，每一个都与一个RNN层匹配：\n",
    "\n",
    "- `keras.layers.SimpleRNNCell`匹配 `SimpleRNN`层.\n",
    "\n",
    "- `keras.layers.GRUCell` 匹配`GRU`层.\n",
    "\n",
    "- `keras.layers.LSTMCell`匹配`LSTM`层.\n",
    "\n",
    "The cell abstraction, together with the generic `keras.layers.RNN` class, make it\n",
    "very easy to implement custom RNN architectures for your research.\n",
    "\n",
    "cell和通用的`keras.layers.RNN`类使得为你的研究实现自定义RNN结构非常容易"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note 学习笔记\n",
    "\n",
    "Still don't understand the 'Mathematically ...' part in the above statement\n",
    "\n",
    "依然就不理解上面的“从数学上来说”这一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60b3b721d500"
   },
   "source": [
    "## Cross-batch statefulness\n",
    "## 跨批次的状态性\n",
    "\n",
    "When processing very long sequences (possibly infinite), you may want to use the\n",
    "pattern of **cross-batch statefulness**.\n",
    "\n",
    "让处理非常长的序列（可能是无限长度的序列），你也许想要使用 **跨批次状态性**模式\n",
    "\n",
    "Normally, the internal state of a RNN layer is reset every time it sees a new batch\n",
    "(i.e. every sample seen by the layer is assumed to be independent of the past). The\n",
    "layer will only maintain a state while processing a given sample.\n",
    "\n",
    "通常，RNN层的初始化状态会在每一次处理新批次的时候重置（比如每一次处理新样本时的层会被认为和之前的处理结果独立）。层只保持当前样本的处理状态\n",
    "\n",
    "If you have very long sequences though, it is useful to break them into shorter\n",
    "sequences, and to feed these shorter sequences sequentially into a RNN layer without\n",
    "resetting the layer's state. That way, the layer can retain information about the\n",
    "entirety of the sequence, even though it's only seeing one sub-sequence at a time.\n",
    "\n",
    "如果你又非常长的序列，将他们分成数个较短的序列，并且将较短的序列在不重置层状态的情况下以此传递给RNN层将非常有用。这样，层可以保留整个序列的信息，及时每次层只能处理一个子序列。\n",
    "\n",
    "You can do this by setting `stateful=True` in the constructor.\n",
    "\n",
    "你可以通过在构建层时设定 `stateful=True`来实现这一功能\n",
    "\n",
    "If you have a sequence `s = [t0, t1, ... t1546, t1547]`, you would split it into e.g.\n",
    "\n",
    "如果你又一个序列`s = [t0, t1, ... t1546, t1547]`，你可以像这样分割它：\n",
    "\n",
    "```\n",
    "s1 = [t0, t1, ... t100]\n",
    "s2 = [t101, ... t201]\n",
    "...\n",
    "s16 = [t1501, ... t1547]\n",
    "```\n",
    "\n",
    "Then you would process it via:\n",
    "然后你可以这样处理这个序列：\n",
    "\n",
    "```python\n",
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "for s in sub_sequences:\n",
    "  output = lstm_layer(s)\n",
    "```\n",
    "\n",
    "When you want to clear the state, you  can use `layer.reset_states()`.\n",
    "当你想要清除状态时，你可以使用`layer.reset_states()`\n",
    "\n",
    "> Note: In this setup, sample `i` in a given batch is assumed to be the continuation of\n",
    "sample `i` in the previous batch. This means that all batches should contain the same\n",
    "number of samples (batch size). E.g. if a batch contains `[sequence_A_from_t0_to_t100,\n",
    " sequence_B_from_t0_to_t100]`, the next batch should contain\n",
    "`[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`.\n",
    "\n",
    "> 注意: 这样设定的话, 给定批次中的样本`i` 将被视为之前批次样本`i`的延续。这意味着所有的批次需要有相同的样本量（批次大小）。比如一个批次含有 `[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`，那么接下来的批次应该包含`[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`。\n",
    "\n",
    "Here is a complete example:\n",
    "\n",
    "这里是一个完整的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:23.497451Z",
     "iopub.status.busy": "2021-03-05T18:08:23.496863Z",
     "iopub.status.idle": "2021-03-05T18:08:32.572252Z",
     "shell.execute_reply": "2021-03-05T18:08:32.572664Z"
    },
    "id": "19e72be49a42"
   },
   "outputs": [],
   "source": [
    "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "\n",
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "output = lstm_layer(paragraph1)\n",
    "output = lstm_layer(paragraph2)\n",
    "output = lstm_layer(paragraph3)\n",
    "\n",
    "# reset_states() will reset the cached state to the original initial_state.\n",
    "# If no initial_state was provided, zero-states will be used by default.\n",
    "# reset_states()将会重置缓存状态到初始状态，如果没有指定初始状态，默认使用0状态\n",
    "lstm_layer.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec7c316b19a1"
   },
   "source": [
    "### RNN State Reuse\n",
    "### RNN 状态复用\n",
    "<a id=\"rnn_state_reuse\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cb7a8ac464a"
   },
   "source": [
    "The recorded states of the RNN layer are not included in the `layer.weights()`. If you\n",
    "would like to reuse the state from a RNN layer, you can retrieve the states value by\n",
    "`layer.states` and use it as the\n",
    "initial state for a new layer via the Keras functional API like `new_layer(inputs,\n",
    "initial_state=layer.states)`, or model subclassing.\n",
    "\n",
    "RNN层所记录的状态并不包括层权重 `layer.weights()`，如果你想要从一个RNN层复用状态，你可以通过`layer.states`提取状态值并通过Keras函数式API `new_layer(inputs, initial_state=layer.states)`或其他模型子类来将其作为初始化状态来用于新层\n",
    "\n",
    "Please also note that sequential model might not be used in this case since it only\n",
    "supports layers with single input and output, the extra input of initial state makes\n",
    "it impossible to use here.\n",
    "\n",
    "同样请注意，这个案例中无法使用顺序模型。这是因为顺序模型只支持一个输入和一个输出的层，初始化状态作为额外输入导致顺序模型无法使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:32.578558Z",
     "iopub.status.busy": "2021-03-05T18:08:32.577971Z",
     "iopub.status.idle": "2021-03-05T18:08:32.608389Z",
     "shell.execute_reply": "2021-03-05T18:08:32.607891Z"
    },
    "id": "009c5b393adf"
   },
   "outputs": [],
   "source": [
    "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "\n",
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "output = lstm_layer(paragraph1)\n",
    "output = lstm_layer(paragraph2)\n",
    "\n",
    "existing_state = lstm_layer.states\n",
    "\n",
    "new_lstm_layer = layers.LSTM(64)\n",
    "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66c1d7f1ccba"
   },
   "source": [
    "## Bidirectional RNNs\n",
    "## 双向RNNs\n",
    "\n",
    "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
    "can perform better if it not only processes sequence from start to end, but also\n",
    "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
    "have the context around the word, not only just the words that come before it.\n",
    "\n",
    "对于非时间序列的次序数据（比如文本数据）而言，如果RNN模型不仅正序处理序列，而且倒序处理，那么模型一般会得到更好的表现。比如说，在预测一句话中的下一个单词时，考察这个单词的上下文将比只考虑这个单词之前的内容更有用\n",
    "\n",
    "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
    "`keras.layers.Bidirectional` wrapper.\n",
    "\n",
    "Keras 提供了一个简单的API来搭建双向RNN：`keras.layers.Bidirectional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:32.614721Z",
     "iopub.status.busy": "2021-03-05T18:08:32.614120Z",
     "iopub.status.idle": "2021-03-05T18:08:33.354607Z",
     "shell.execute_reply": "2021-03-05T18:08:33.354118Z"
    },
    "id": "8cea1781a0c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 5, 128)            38400     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 80,266\n",
      "Trainable params: 80,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
    ")\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dab57c97a566"
   },
   "source": [
    "Under the hood, `Bidirectional` will copy the RNN layer passed in, and flip the\n",
    "`go_backwards` field of the newly copied layer, so that it will process the inputs in\n",
    "reverse order.\n",
    "\n",
    "The output of the `Bidirectional` RNN will be, by default, the concatenation of the forward layer\n",
    "output and the backward layer output. If you need a different merging behavior, e.g.\n",
    "concatenation, change the `merge_mode` parameter in the `Bidirectional` wrapper\n",
    "constructor. For more details about `Bidirectional`, please check\n",
    "[the API docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional/).\n",
    "\n",
    "模型中的`Bidirectional`会复制传递进来的RNN层，并将新的复制层的`go_backwards`部分翻转，这样就可以处理反向的输入了\n",
    "\n",
    "在默认情况下，`Bidirectional` RNN的输出会是正向输出和反向输出的组合。如果你需要不同的合并方法，可以更改`Bidirectional`中的`merge_mode`参数。更多关于`Bidirectional`的信息，请参考[API文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18a254dfaa73"
   },
   "source": [
    "## Performance optimization and CuDNN kernels\n",
    "## 表现优化和CuDNN内核\n",
    "\n",
    "In TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN\n",
    "kernels by default when a GPU is available. With this change, the prior\n",
    "`keras.layers.CuDNNLSTM/CuDNNGRU` layers have been deprecated, and you can build your\n",
    "model without worrying about the hardware it will run on.\n",
    "\n",
    "在TensorFlow 2.0，内置的LSTM和GRU层已经升级以支持CuDNN内核。当GPU可用的情况下，将默认使用CuDNN内核。因为这个更新，之前的`keras.layers.CuDNNLSTM/CuDNNGRU`层已经被弃用。你现在可以构建你的模型而不用担心硬件问题了\n",
    "\n",
    "Since the CuDNN kernel is built with certain assumptions, this means the layer **will\n",
    "not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or\n",
    "GRU layers**. E.g.:\n",
    "\n",
    "- Changing the `activation` function from `tanh` to something else.\n",
    "- Changing the `recurrent_activation` function from `sigmoid` to something else.\n",
    "- Using `recurrent_dropout` > 0.\n",
    "- Setting `unroll` to True, which forces LSTM/GRU to decompose the inner\n",
    "`tf.while_loop` into an unrolled `for` loop.\n",
    "- Setting `use_bias` to False.\n",
    "- Using masking when the input data is not strictly right padded (if the mask\n",
    "corresponds to strictly right padded data, CuDNN can still be used. This is the most\n",
    "common case).\n",
    "\n",
    "由于CuDNN内核实在某些假设的前提下建立的，因此层 **将无法使用CuDNN内核，如果你改变了内置LSTM或者GRU层的默认设定的话**比如：\n",
    "- 将激活函数`activation` 从`tanh` 改为其他函数.\n",
    "- 将循环激活函数`recurrent_activation` 从`sigmoid` 改为其他.\n",
    "- 设置 `recurrent_dropout` > 0.\n",
    "- 设置 `unroll` 为真, 这导致LSTM/GRU将内部的`tf.while_loop`循环分解为一个展开的`for`循环\n",
    "- 设置 `use_bias` 为假.\n",
    "- 当输入数据不是严格的向右填充时使用遮罩（如果数据时严格的向右填充，那么CuDNN内核依然可用。这是最常见的情况）\n",
    "\n",
    "更多细节，请参见：\n",
    "[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM/) 和\n",
    "[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU/) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8de09c4343"
   },
   "source": [
    "### Using CuDNN kernels when available\n",
    "### 使用CuDNN内核\n",
    "\n",
    "Let's build a simple LSTM model to demonstrate the performance difference.\n",
    "\n",
    "We'll use as input sequences the sequence of rows of MNIST digits (treating each row of\n",
    "pixels as a timestep), and we'll predict the digit's label.\n",
    "\n",
    "让我们建立一个简单的LSTM模型来表明性能上的差异\n",
    "\n",
    "我们将使用MNIST数字的行序列作为输入序列（将每一个像素列作为时间段），然后预测数字的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:33.361060Z",
     "iopub.status.busy": "2021-03-05T18:08:33.360486Z",
     "iopub.status.idle": "2021-03-05T18:08:33.362314Z",
     "shell.execute_reply": "2021-03-05T18:08:33.362667Z"
    },
    "id": "e88aab9e73c7"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
    "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
    "# 每一个MNIST图像批量是一个（批量大小，28,28）张量，每一个输入序列的维度是（28,28）\n",
    "input_dim = 28\n",
    "\n",
    "units = 64\n",
    "output_size = 10  # labels are from 0 to 9\n",
    "\n",
    "# Build the RNN model\n",
    "# 建立RNN模型\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    # CuDNN只适用于层二不适用于cell。这意味着`LSTM(units)`将使用CuDNN内核，同时RNN(LSTMCell(units))将使用非CuDNN内核\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "    else:\n",
    "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "        lstm_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
    "        )\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(output_size),\n",
    "        ]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcde82cb14d6"
   },
   "source": [
    "Let's load the MNIST dataset:\n",
    "\n",
    "让我们载入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:33.367221Z",
     "iopub.status.busy": "2021-03-05T18:08:33.366588Z",
     "iopub.status.idle": "2021-03-05T18:08:34.246634Z",
     "shell.execute_reply": "2021-03-05T18:08:34.245962Z"
    },
    "id": "98292f8e71a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "sample, sample_label = x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "443e5458284f"
   },
   "source": [
    "Let's create a model instance and train it.\n",
    "\n",
    "We choose `sparse_categorical_crossentropy` as the loss function for the model. The\n",
    "output of the model has shape of `[batch_size, 10]`. The target for the model is an\n",
    "integer vector, each of the integer is in the range of 0 to 9.\n",
    "\n",
    "让我们创造一个模型实例并训练它\n",
    "\n",
    "我们选择`sparse_categorical_crossentropy`作为模型的损失函数，模型的输出形式是 `[batch_size, 10]`。模型的目标是一个整数向量，每一个整数的取值范围是0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:34.260641Z",
     "iopub.status.busy": "2021-03-05T18:08:34.259932Z",
     "iopub.status.idle": "2021-03-05T18:08:40.992358Z",
     "shell.execute_reply": "2021-03-05T18:08:40.992760Z"
    },
    "id": "f85b57b010e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 6s 5ms/step - loss: 1.2900 - accuracy: 0.5779 - val_loss: 0.5115 - val_accuracy: 0.8402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5cf00c6518>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(allow_cudnn_kernel=True)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99ea5495e375"
   },
   "source": [
    "Now, let's compare to a model that does not use the CuDNN kernel:\n",
    "\n",
    "现在，让我们与不使用CuDNN内核的模型比较一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:41.005565Z",
     "iopub.status.busy": "2021-03-05T18:08:41.004969Z",
     "iopub.status.idle": "2021-03-05T18:09:13.339270Z",
     "shell.execute_reply": "2021-03-05T18:09:13.338717Z"
    },
    "id": "d4bdff02e617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 32s 33ms/step - loss: 0.4511 - accuracy: 0.8632 - val_loss: 0.3877 - val_accuracy: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5ce00c5748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
    "noncudnn_model.set_weights(model.get_weights())\n",
    "noncudnn_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "noncudnn_model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90fc64fbd4ea"
   },
   "source": [
    "When running on a machine with a NVIDIA GPU and CuDNN installed,\n",
    "the model built with CuDNN is much faster to train compared to the\n",
    "model that uses the regular TensorFlow kernel.\n",
    "\n",
    "The same CuDNN-enabled model can also be used to run inference in a CPU-only\n",
    "environment. The `tf.device` annotation below is just forcing the device placement.\n",
    "The model will run on CPU by default if no GPU is available.\n",
    "\n",
    "You simply don't have to worry about the hardware you're running on anymore. Isn't that\n",
    "pretty cool?\n",
    "\n",
    "当在装备了NVIDIA显卡和CuDNN的设备上运行时，相比于传统的TensorFlow内核，CuDNN加速模型有更快的训练速度。\n",
    "\n",
    "同样的CuDNN支持模型可以被用于在只使用CPU的环境下进行推测。下面的`tf.device`注释强制指定使用的内核。如果GPU不可用，那么模型也会默认采用CPU来运行\n",
    "\n",
    "你不再需要为硬件情况的担心了，这是不是非常酷？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:09:13.345157Z",
     "iopub.status.busy": "2021-03-05T18:09:13.344502Z",
     "iopub.status.idle": "2021-03-05T18:09:14.663386Z",
     "shell.execute_reply": "2021-03-05T18:09:14.663763Z"
    },
    "id": "7e33c62b6029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result is: [3], target result is: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
    "    cpu_model.set_weights(model.get_weights())\n",
    "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
    "    print(\n",
    "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
    "    )\n",
    "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f940b73a2a6"
   },
   "source": [
    "## RNNs with list/dict inputs, or nested inputs\n",
    "## 采用列表/字典或者嵌套输入的RNN模型\n",
    "\n",
    "Nested structures allow implementers to include more information within a single\n",
    "timestep. For example, a video frame could have audio and video input at the same\n",
    "time. The data shape in this case could be:\n",
    "\n",
    "嵌套结构允许实施者在单个时间段内输入更多的信息，比如说，一个视频框架可以同时包含视频和音频输入。这样数据的形式就是\n",
    "\n",
    "`[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]`\n",
    "\n",
    "In another example, handwriting data could have both coordinates x and y for the\n",
    "current position of the pen, as well as pressure information. So the data\n",
    "representation could be:\n",
    "\n",
    "另一个例子是，手写数据包含手写笔在X轴和Y轴上的位置信息和压力信息，所以这种数据会表示为\n",
    "\n",
    "`[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]`\n",
    "\n",
    "The following code provides an example of how to build a custom RNN cell that accepts\n",
    "such structured inputs.\n",
    "\n",
    "接下来的代码提供了一个关于五河建立自定义RNN cell来接受结构化输入的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f78dc4c1c516"
   },
   "source": [
    "### Define a custom cell that supports nested input/output\n",
    "### 自定义cell来支持嵌套输入/输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "199faf57f0c5"
   },
   "source": [
    "See [Making new Layers & Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models/)\n",
    "for details on writing your own layers.\n",
    "\n",
    "参考[通过子类来建立新层和新模型](https://www.tensorflow.org/guide/keras/custom_layers_and_models/) 来了解更多细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:09:14.673544Z",
     "iopub.status.busy": "2021-03-05T18:09:14.672972Z",
     "iopub.status.idle": "2021-03-05T18:09:14.674855Z",
     "shell.execute_reply": "2021-03-05T18:09:14.675298Z"
    },
    "id": "451cfd5f0cc4"
   },
   "outputs": [],
   "source": [
    "class NestedCell(keras.layers.Layer):\n",
    "    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\n",
    "        self.unit_1 = unit_1\n",
    "        self.unit_2 = unit_2\n",
    "        self.unit_3 = unit_3\n",
    "        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
    "        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
    "        super(NestedCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n",
    "        i1 = input_shapes[0][1]\n",
    "        i2 = input_shapes[1][1]\n",
    "        i3 = input_shapes[1][2]\n",
    "\n",
    "        self.kernel_1 = self.add_weight(\n",
    "            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\n",
    "        )\n",
    "        self.kernel_2_3 = self.add_weight(\n",
    "            shape=(i2, i3, self.unit_2, self.unit_3),\n",
    "            initializer=\"uniform\",\n",
    "            name=\"kernel_2_3\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n",
    "        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n",
    "        input_1, input_2 = tf.nest.flatten(inputs)\n",
    "        s1, s2 = states\n",
    "\n",
    "        output_1 = tf.matmul(input_1, self.kernel_1)\n",
    "        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\n",
    "        state_1 = s1 + output_1\n",
    "        state_2_3 = s2 + output_2_3\n",
    "\n",
    "        output = (output_1, output_2_3)\n",
    "        new_states = (state_1, state_2_3)\n",
    "\n",
    "        return output, new_states\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51355b4089d2"
   },
   "source": [
    "### Build a RNN model with nested input/output\n",
    "### 建立一个使用嵌套输入/输出的RNN模型\n",
    "\n",
    "Let's build a Keras model that uses a `keras.layers.RNN` layer and the custom cell\n",
    "we just defined.\n",
    "\n",
    "让我们用`keras.layers.RNN`来建立一个Keras模型并自定义我们所定义的cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:09:14.688724Z",
     "iopub.status.busy": "2021-03-05T18:09:14.688114Z",
     "iopub.status.idle": "2021-03-05T18:09:14.802944Z",
     "shell.execute_reply": "2021-03-05T18:09:14.802448Z"
    },
    "id": "b2eba7a248eb"
   },
   "outputs": [],
   "source": [
    "unit_1 = 10\n",
    "unit_2 = 20\n",
    "unit_3 = 30\n",
    "\n",
    "i1 = 32\n",
    "i2 = 64\n",
    "i3 = 32\n",
    "batch_size = 64\n",
    "num_batches = 10\n",
    "timestep = 50\n",
    "\n",
    "cell = NestedCell(unit_1, unit_2, unit_3)\n",
    "rnn = keras.layers.RNN(cell)\n",
    "\n",
    "input_1 = keras.Input((None, i1))\n",
    "input_2 = keras.Input((None, i2, i3))\n",
    "\n",
    "outputs = rnn((input_1, input_2))\n",
    "\n",
    "model = keras.models.Model([input_1, input_2], outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "452a99c63b7c"
   },
   "source": [
    "### Train the model with randomly generated data\n",
    "### 用随机生成的数据来训练这个模型\n",
    "\n",
    "Since there isn't a good candidate dataset for this model, we use random Numpy data for\n",
    "demonstration.\n",
    "\n",
    "由于这个模型没有好的候选数据集，所以我们用随机numpy数据来进行演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:09:14.807823Z",
     "iopub.status.busy": "2021-03-05T18:09:14.807248Z",
     "iopub.status.idle": "2021-03-05T18:09:16.652281Z",
     "shell.execute_reply": "2021-03-05T18:09:16.651794Z"
    },
    "id": "3987993cb7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 25ms/step - loss: 0.9178 - rnn_1_loss: 0.3211 - rnn_1_1_loss: 0.5968 - rnn_1_accuracy: 0.1097 - rnn_1_1_accuracy: 0.0382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5c391e43c8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1_data = np.random.random((batch_size * num_batches, timestep, i1))\n",
    "input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3))\n",
    "target_1_data = np.random.random((batch_size * num_batches, unit_1))\n",
    "target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3))\n",
    "input_data = [input_1_data, input_2_data]\n",
    "target_data = [target_1_data, target_2_data]\n",
    "\n",
    "model.fit(input_data, target_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b51e87780b0f"
   },
   "source": [
    "With the Keras `keras.layers.RNN` layer, You are only expected to define the math\n",
    "logic for individual step within the sequence, and the `keras.layers.RNN` layer\n",
    "will handle the sequence iteration for you. It's an incredibly powerful way to quickly\n",
    "prototype new kinds of RNNs (e.g. a LSTM variant).\n",
    "\n",
    "For more details, please visit the [API docs](https://https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN/).\n",
    "\n",
    "使用`keras.layers.RNN`层，你只需要为序列中的各个步骤定义数据逻辑。同时，`keras.layers.RNN`层还可以为了管控序列迭代。 这是一种快速建立新的RNN原型的强大方式\n",
    "\n",
    "更多细节，请访问[API 文档](https://https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
